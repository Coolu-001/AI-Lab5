Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/50: 100%|██████████████████████████████████| 200/200 [14:12<00:00,  4.26s/it, loss=1.1075, lr=1.00e-05]
Evaluating: 100%|██████████████████████████████████████████████████████████████| 50/50 [00:34<00:00,  1.44it/s]
Epoch 2/50: 100%|██████████████████████████████████| 200/200 [14:55<00:00,  4.48s/it, loss=1.1198, lr=2.00e-05]
Evaluating: 100%|██████████████████████████████████████████████████████████████| 50/50 [00:35<00:00,  1.40it/s]
Epoch 3/50: 100%|██████████████████████████████████| 200/200 [14:22<00:00,  4.31s/it, loss=1.0964, lr=3.00e-05]
Evaluating: 100%|██████████████████████████████████████████████████████████████| 50/50 [00:34<00:00,  1.44it/s]
EarlyStopping counter: 1 out of 5
Epoch 4/50: 100%|██████████████████████████████████| 200/200 [14:48<00:00,  4.44s/it, loss=1.0211, lr=4.00e-05]
Evaluating: 100%|██████████████████████████████████████████████████████████████| 50/50 [00:34<00:00,  1.45it/s]
EarlyStopping counter: 2 out of 5
Epoch 5/50:  54%|██████████████████▎               | 108/200 [08:04<06:52,  4.48s/it, loss=0.8104, lr=4.54e-05]
Traceback (most recent call last):
  File "/Users/caolu/Desktop/AI-Lab5-clip-latefusion/main.py", line 114, in <module>
    val_accuracy = trainer.train(train_dataloader, valid_dataloader, config.epochs, evaluate_every=1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/caolu/Desktop/AI-Lab5-clip-latefusion/train_validate.py", line 106, in train
    self.optimizer.step()
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/Users/caolu/opt/anaconda3/envs/multi/lib/python3.11/site-packages/torch/optim/adamw.py", line 376, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
    ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
